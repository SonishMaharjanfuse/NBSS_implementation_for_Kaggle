seed_everything: 2
trainer:
  gradient_clip_val: 5
  gradient_clip_algorithm: norm
  accumulate_grad_batches: null
  strategy: ddp_find_unused_parameters_false
  sync_batchnorm: false
  precision: 32
model:
  arch:
    class_path: models.arch.blstm2_fc1.BLSTM2_FC1
    init_args:
      activation: ""
      hidden_size:
        - 256
        - 128
      n_repeat_last_lstm: 1
      dropout: null
  io:
    class_path: models.io.narrow_band.td_signal_nb.TimeDomainSignalNB
    init_args:
      ft_len: 512
      ft_overlap: 256
      loss_func: models.io.narrow_band.td_signal_nb.neg_si_sdr
      loss_name: null
  ref_channel: 0
  channels: [0, 1, 2, 3, 4, 5, 6, 7]
  learning_rate: 0.001
  optimizer: Adam
  optimizer_kwargs: {}
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_kwargs:
    mode: min
    factor: 0.5
    patience: 10
    min_lr: 0.0001
  exp_name: exp
  metrics: [SDR, SI_SDR, NB_PESQ, WB_PESQ]
data:
  class_path: data_loaders.SS_SemiOnlineDataModule
  init_args:
    clean_speech_dataset: wsj0
    clean_speech_dir: /kaggle/input/wsj0-2mix/
    rir_dir: /kaggle/input/nbss-data/implementation_for_Kaggle/dataset
    speech_overlap_ratio:
      - 0.1
      - 1.0
    speech_scale:
      - -5.0
      - 5.0
    batch_size:
      - 15
      - 15
    speaker_num: 5
    audio_time_len:
      - headtail 4
      - headtail 4
      - headtail 4
    num_workers: 15
    test_set: test
    shuffle_train_rir: true
    seeds:
      train: null
      val: 2
      test: 3
    pin_memory: true
    prefetch_factor: 5
early_stopping:
  monitor: val/neg_si_sdr
  min_delta: 0.01
  patience: 30
  verbose: false
  mode: min
  strict: true
  check_finite: true
  stopping_threshold: null
  divergence_threshold: null
  check_on_train_epoch_end: null
model_checkpoint:
  dirpath: null
  filename: epoch{epoch}_neg_si_sdr{val/neg_si_sdr:.4f}
  monitor: val/neg_si_sdr
  verbose: false
  save_last: true
  save_top_k: 5
  save_weights_only: false
  mode: min
  auto_insert_metric_name: false
  every_n_train_steps: null
  train_time_interval: null
  every_n_epochs: 1
  save_on_train_epoch_end: null
learning_rate_monitor:
  logging_interval: epoch
  log_momentum: false
# model_summary:
#   max_depth: -1
ckpt_path: null
